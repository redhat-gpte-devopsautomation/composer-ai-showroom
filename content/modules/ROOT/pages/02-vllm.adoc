= Deploying a Model with vLLM

In this section will will deploy a https://huggingface.co/ibm-granite/granite-3.0-8b-instruct[Granite 3.0 8B Instruct] model using vLLM.

For our Model Server we will be deploying a vLLM instance using a model packaged into an OCI container with ModelCar.

[NOTE]
====
ModelCar is Tech Preview as of OpenShift AI 2.14.

ModelCar is a great option for smaller models like our 8B model.  While it is still a relatively large container (15Gb) it is still reasonable to easily pull into a cluster.

Treating the model as an OCI artifact allows us to easily promote the model between different environments using customers existing promotion processes.  By contrast, dealing with promoting models between S3 instances in different environments may create new challenges.
====

== Creating the vLLM Instance

. Open the https://rhods-dashboard-redhat-ods-applications.{openshift_cluster_ingress_domain}[OpenShift AI Dashboard] and select the `composer-ai-apps` project from the list of Data Science Projects

+
image::02-composer-ai-apps-project.png[Composer AI Apps Project]

. Select the `Models` tab and click `Deploy model`

+
image::02-deploy-model.png[Deploy Model]

. Enter the following information

+
[source,properties]
----
Model deployment name: vllm
Serving runtime: vLLM ServingRuntime for KServe
Model server size: Custom
CPUs requested: 2 Cores
CPUs limit: 4 Cores
Memory requested: 16 GiB
Memory limit: 20 GiB
Accelerator: nvidia-gpu
Number of accelerators: 1
----

+
image::02-model-options.png[Model Options]

. In the `Source model location` section, choose the option to `Create connection`.  Enter the following information:

+
[source,properties]
----
Connection type: URI - v1
Connection name: granite-3-0-8b-instruct
URI: oci://image-registry.openshift-image-registry.svc:5000/composer-ai-apps/granite-3.0-8b-instruct:latest
----

+
image::02-uri-connection.png[URI Connection]

+
[NOTE]
====
A copy of the image with our model has already been loaded onto the cluster as an ImageStream to help speed up the process of pulling the image.

However, you can find the original image https://github.com/redhat-ai-services/modelcar-catalog/[here] alongside other ModelCar images that you can try.

Additionally, the source for building these ModelCar images can be found on https://github.com/redhat-ai-services/modelcar-catalog/[GitHub].
====

. A new vLLM instance will be created in the OpenShift AI Dashboard.  Return to the OpenShift Web Console and check the pods in the `composer-ai-apps` project.  You should find a pod called `vllm-predictor-00001-deployment-*`.  Check the pods `Events` and `Logs` to follow the progress for the pod until it becomes ready.

. (Optional) The OpenShift AI Dashboard created two KServe objects, a `ServingRuntime` and an `InferenceService`.  From the OpenShift Web Console, navigate to the `Home` > `Search` page and use the `Resources` drop down menu to search for and select those objects.  Spend a few minutes reviewing the objects created by the Dashboard.

+
image::02-kserve-objects.png[KServe Objects]

== Testing vLLM Endpoints

The vLLM instance may take a while to pull the model image and load it.  Feel free to move onto the next section and come back to test the endpoint once the vLLM instance is up and running.
