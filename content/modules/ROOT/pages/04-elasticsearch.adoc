= Deploying a Vector Database with Elasticsearch

Now that our LLM is up and running, we now need to deploy a vector database that we will use to ingest documents for our RAG architecture.  In our workshop we will be utilizing Elasticsearch for our vector database.

Elasticsearch is a Red Hat partner and Red Hat has announced future integrations within OpenShift AI.

== Creating the Elasticsearch Instance

The Elasticsearch (ECK) Operator has already been installed on the cluster for you, so we will just need to create an `Elasticsearch Cluster` instance.

. With the `composer-ai-apps` namespace selected in the OpenShift Web Console, navigate to `Operators` > `Installed Operators` and select the `Elasticsearch (ECK) Operator`.

+
image::04-elasticsearch-operator.png[Elasticsearch Operator]

. Click the `+Create instance` option for an `Elasticsearch Cluster`

+
image::04-elasticsearch-operator.png[Elasticsearch Operator]

. Update the object name to `elasticsearch` and click `Create`.

+
image::04-elasticsearch-yaml.png[Elasticsearch YAML]

Three new Elasticsearch pods will be created in the `composer-ai-apps` namespace.  Once they are Ready, you are ready to move on to the next step.

== Populating Vector Database

Now that we have the elasticsearch cluster up and running, we must ingest documents into it.  This will allow us to create Retrieval Augmented Generation (RAG) based assistants that Composer AI can use.

[NOTE]
====
For demonstration purposes we will be ingesting documentation for various Red Hat products.  However, it is import to keep in mind that basically any data source can be used in the context of RAG, including the live web.  If you have ever used the chatbot Perplexity AI, that is what it is doing behind the scenes. 
====

# Instructions for executing the ingestion pipeline

To get you started, we have included an ingestion pipeline to populate the elasticsearch instance with information pulled from openshift documentation.  Go to pipelines in the composer-ai-apps namespaces.  Run the `ingestion-pipeline` pipeline with the default parameters.  Set the `source` workspace to the VolumeClaimTemplate option.  

image::04-show-pipeline-1.png[Instantiate Template]

image::04-show-pipeline-2.png[Instantiate Template]

The progress can be viewed in the "Experiments and runs" section in the Openshift AI console.

image::04-view-experiments.png[Instantiate Template]

From here you can view the progress of the Kubeflow Pipeline (KFP), as well as the logs of individual steps, similar to Tekton.  
