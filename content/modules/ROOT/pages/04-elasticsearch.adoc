= Deploying a Vector Database with Elasticsearch for Retrieval Augmented Generation (RAG)

== Working with RAG

We've explored using system prompts to guide LLM responses, but prompting alone has limitations.  The following sections detail how to set up https://www.redhat.com/en/topics/ai/what-is-retrieval-augmented-generation[Retrieval Augmented Generation (RAG)], a cost-effective method for enhancing LLM responses with more relevant data.

RAG significantly improves LLM responses by providing pertinent information related to a user's query, rather than relying solely on the LLM's training data.

== Why Vector Databases?

RAG requires a data source.  Currently, vector databases are the most common solution for this purpose.  They enable us to divide data into logical "chunks" and then use tools like LangChain to query the database for chunks related to a specific question. These relevant chunks are then provided to the LLM as context, improving the quality of its response.

This workshop uses Elasticsearch as our vector database.  As a Red Hat partner, Elasticsearch will be integrated with OpenShift AI in the future.

== Creating our Elasticsearch Instance

The Elasticsearch (ECK) Operator has already been installed on the cluster for you, so we will just need to create an `Elasticsearch Cluster` instance.

. With the `composer-ai-apps` namespace selected in the OpenShift Web Console, navigate to `Operators` > `Installed Operators` and select the `Elasticsearch (ECK) Operator`.

+
image::04-elasticsearch-operator.png[Elasticsearch Operator]

. Click the `+Create instance` option for an `Elasticsearch Cluster`

+
image::04-create-elasticsearch-cluster.png[Create Elasticsearch Cluster]

. Update the object name to `elasticsearch`.

. Add the following yaml to the top level `spec:` of the `Elasticsearch` custom resource:

+
```
spec:
  http:
    tls:
      selfSignedCertificate:
        disabled: true
```
and then click `Create`.


+
image::04-elasticsearch-yaml.png[Elasticsearch YAML]

. Wait until the Elasticsearch Cluster has come up
+
This can be validated by checking the opeartor status or validating that the Elasticsearch StatefulSet is up and running in the `composer-ai-apps` namespace.

. Finally, navigate to `Workloads -> Pods` and delete our `quarkus-llm-router-` pod.

NOTE:  Elasticsearch creates the secret `elasticsearch-es-elastic-user` that contains a password that our Conductor(quarkus-llm-router) app requires in order to authenticate against the ElasticSearch instance

IMPORTANT: A placeholder `elasticsearch-es-elastic-user` secret was created with the password `REPLACE_ME`. Ensure that the operator has updated this value *before* deleting the `quarkus-llm-router-` pod.

== Populating out data

Now that we have the elasticsearch cluster up and running, lets add data by ingesting documents.

[TIP]
====
For this demonstration, we'll use Red Hat product documentation.  Keep in mind, however, that Retrieval Augmented Generation (RAG) can use almost any data source, including live web data.  Perplexity AI's chatbot works in a similar way.
====

=== Executing the ingestion pipeline

. From the `composer-ai-apps` namespace inside of the https://console-openshift-console.{openshift_cluster_ingress_domain}/pipelines/ns/composer-ai-apps[OpenShift Web Console] navigate to the `Pipelines` > `Pipelines` section and locate the `ingestion-pipeline` pipeline.
+
Click on the `...` on the right and click `Start`.

+
image::04-start-pipeline.png[Start Pipeline]

. Leave all of the default options except for the `source`. 
+
Set `source` to `VolumeClaimTemplate` and start the pipeline.

+
image::04-volume-claim-template.png[Volume Claim Template]

. This Tekton pipeline will create and initiate the data science pipelines within RHOAI.
+
Wait for the execute-kubeflow-pipeline task to complete.

+
image::04-pipeline-run.png[Pipeline Run]

. A new Data Science Pipeline should have been created and started.
+
To view the execution, navigate to the OpenShift AI Dashboard and select `Experiments` > `Experiments and runs`.
+
Click on the `document_ingestion` experiment. 

+
NOTE: Make sure you are in the `composer-ai-apps` Project.

+
image::04-view-experiments.png[View experiments]

. You should see a single run listed.  Click on the run to view the execution of that run.

+
image::04-view-run.png[View run]

+
The data ingestion process may take several minutes to complete. You can follow the progress of the various steps by checking the `Logs` for each step in the OpenShift AI Dashboard.
+
Behind the scenes this process is creating pods in the OpenShift Web Console and tailing those logs, meaning this information can also be captured by normal logging processes.
+
Some of the steps require a GPU which will trigger a new GPU node to be auto-scaled in your cluster, which can take several minutes.
+
Feel free to start reading into the next section

+
[NOTE]
====
The data ingestion pipeline currently uses a basic chunking algorithm (see the https://github.com/redhat-composer-ai/data-ingestion/blob/main/kfp/redhat-product-documentation-ingestor/ingestion-pipeline-elastic.py[code]).  We are planning future enhancements to make the pipeline more flexible and to improve the chunking algorithm.  Specifically, we are investigating the use of https://ds4sd.github.io/docling/[docling] which should lead to better results.
====

=== Testing the Vector Database from Composer AI UI

Now that our vector database is up and running and we have ingested some documents.

We have already created several Assistants that are ready to use the documents we have ingested.

. Navigate to the Composer AI UI and click the `Compare` button in the top right hand corner.

+
[NOTE]
====
The `Compare` interface provides the ability to query multiple assistants at the same time to allow a user to compare the results.  This can be a useful feature for testing various assistants to see how the responses differ.
====

. Select the `Default Red Hat OpenShift AI Self Managed Assistant` for the left chat, and the `Default Assistant` we used before to test our LLM.  Ask a question about OpenShift AI such as "How do I create a data connection?" and see how the responses change with our RAG workflow.

+
image::04-compare.png[Compare Assistant]
